3/7/2019
### 不同activation比较
    relu比tanh收敛更快
    1. 从函数图形的梯度来解释，tanh在函数值稍大的地方开始梯度就很小，出现梯度消失现象
    2. relu不存在这个现象，而且大多数值都不会为负值。

### 不同optimizer比较
    1. adadelta效果最好
    2. adam也可以
    3. sgd最快，快速验证想法的是由用sgd
    最好的效果应该是SGD+随着训练调节学习率
    


